AWSTemplateFormatVersion: "2010-09-09"

Description: Amazon Transcribe Live Call Analytics - LCA SSM Parameters

Parameters:
  # Required
  LCAStackName:
    Type: String
    Description: LCA Stack Name

  LLMPromptSummaryTemplate:
    Type: String
    Description: >-
      Prompt to use to generate insights for a call. This can be a single string where an LLM returns a string,
      or a single string where the LLM returns a JSON object with key/value pairs, or a string that contains
      a JSON Object with key/value pairs, where the LLM will run one inference on each key/value pair with the value
      containing the prompt. Use {transcript} as a placeholder for where the call transcript will be injected.
    Default: >-
      Human: Answer all the questions below as a json object with key value pairs, based on the transcript. Use the text before the colon as the key. Only return json. Use gender neutral pronouns. Skip the preamble; go straight into the json.
      <br><questions> 
      <br>Summary: Write a short summary of the transcript.
      <br>Topics: What are the topics discussed in the transcript, as a comma separated string. Choose from one or more of the following (iphone issue, billing issue, insurance, cancellation), or create new topics.
      <br>Follow-up Actions: What are the follow-up actions, as a comma separated string, that the agent is going to perform based on the transcript.
      <br></questions>  
      <br><transcript> 
      <br>{transcript} 
      <br></transcript> 
      <br>Assistant: 

Resources:
  LLMPromptSummaryTemplateParameter:
    Type: "AWS::SSM::Parameter"
    Properties:
      Name: !Sub "${LCAStackName}-LLMPromptSummaryTemplate"
      Type: String
      Description: >
        Prompt to use to generate insights for a call. This can be a single string where an LLM returns a string,
        or a single string where the LLM returns a JSON object with key/value pairs, or a string that contains
        a JSON Object with key/value pairs, where the LLM will run one inference on each key/value pair with the value
        containing the prompt. Use {transcript} as a placeholder for where the call transcript will be injected.
      Value: !Ref LLMPromptSummaryTemplate

Outputs:
  LLMPromptSummaryTemplateParameter:
    Value: !Ref LLMPromptSummaryTemplateParameter